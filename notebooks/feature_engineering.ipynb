{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa50cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d23c071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c226421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "NUMERICAL_COLS = ['Amount', 'Value', 'CountryCode', 'PricingStrategy']\n",
    "CATEGORICAL_COLS = ['CurrencyCode', 'ProductCategory', 'ChannelId', 'ProviderId']  # Removed 'ProductId'\n",
    "REMAINDER_COLS = ['TransactionId', 'BatchId', 'AccountId', 'SubscriptionId', 'CustomerId']\n",
    "IMPUTE_CATEGORICAL_COLS = ['CurrencyCode', 'ProductCategory', 'ChannelId', 'ProviderId']  # Exclude ProductId\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ce65d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregateFeatures:\n",
    "    \"\"\"Custom transformer to compute aggregate features per CustomerId.\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        logging.info(\"Computing aggregate features...\")\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "        if 'CustomerId' not in X.columns or 'Amount' not in X.columns:\n",
    "            raise ValueError(\"Required columns 'CustomerId' and 'Amount' not found.\")\n",
    "        agg_df = X.groupby('CustomerId').agg({\n",
    "            'Amount': ['sum', 'mean', 'count', 'std'],\n",
    "        }).reset_index()\n",
    "        agg_df.columns = ['CustomerId', 'TotalAmount', 'AvgAmount', 'TransactionCount', 'StdAmount']\n",
    "        agg_df['StdAmount'] = agg_df['StdAmount'].fillna(0)\n",
    "        X = X.merge(agg_df, on='CustomerId', how='left')\n",
    "        logging.info(f\"Columns after AggregateFeatures: {list(X.columns)}\")\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0f1cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatetimeFeatures:\n",
    "    \"\"\"Custom transformer to extract datetime features from TransactionStartTime.\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        logging.info(\"Extracting datetime features...\")\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "        if 'TransactionStartTime' not in X.columns:\n",
    "            raise ValueError(\"Required column 'TransactionStartTime' not found.\")\n",
    "        X = X.copy()\n",
    "        X['TransactionStartTime'] = pd.to_datetime(X['TransactionStartTime'], errors='coerce')\n",
    "        X['TransactionHour'] = X['TransactionStartTime'].dt.hour\n",
    "        X['TransactionDay'] = X['TransactionStartTime'].dt.day\n",
    "        X['TransactionMonth'] = X['TransactionStartTime'].dt.month\n",
    "        X['TransactionYear'] = X['TransactionStartTime'].dt.year\n",
    "        logging.info(f\"Columns after DatetimeFeatures: {list(X.columns)}\")\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6baaffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_engineering_pipeline():\n",
    "    \"\"\"Creates a feature engineering pipeline for transaction data.\"\"\"\n",
    "    logging.info(\"Validating input columns...\")\n",
    "    expected_cols = NUMERICAL_COLS + CATEGORICAL_COLS + REMAINDER_COLS + ['TransactionStartTime']\n",
    "    \n",
    "    numerical_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_pipeline_impute = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    categorical_pipeline_no_impute = Pipeline([\n",
    "        ('onehot', OneHotEncoder(handle_unknown='error', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', numerical_pipeline, NUMERICAL_COLS),\n",
    "        ('cat_impute', categorical_pipeline_impute, IMPUTE_CATEGORICAL_COLS),\n",
    "        ('cat_no_impute', categorical_pipeline_no_impute, ['ProductId'])\n",
    "    ], remainder='passthrough')\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('aggregate', AggregateFeatures()),\n",
    "        ('datetime', DatetimeFeatures()),\n",
    "        ('preprocessor', preprocessor)\n",
    "    ])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "715b9ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(input_path, output_path, target_col='FraudResult'):\n",
    "    \"\"\"Processes raw data and saves transformed data.\"\"\"\n",
    "    # Validate input file\n",
    "    logging.info(f\"Checking for input file at {input_path}...\")\n",
    "    if not os.path.isfile(input_path):\n",
    "        logging.warning(f\"Input file {input_path} not found.\")\n",
    "        # Fallback to user input if file not found\n",
    "        input_path = input(\"Please enter the correct path to data.csv: \").strip()\n",
    "        if not os.path.isfile(input_path):\n",
    "            logging.error(f\"Provided file {input_path} still not found.\")\n",
    "            raise FileNotFoundError(f\"File {input_path} does not exist.\")\n",
    "    \n",
    "    logging.info(f\"Loading data from {input_path}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(input_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load {input_path}: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    # Log dataset info for debugging\n",
    "    logging.info(f\"Dataset info:\\n{df.info()}\")\n",
    "    logging.info(f\"ProviderId unique values: {df['ProviderId'].nunique()}\")\n",
    "    logging.info(f\"ProductId unique values: {df['ProductId'].nunique()}\")\n",
    "    logging.info(f\"FraudResult distribution:\\n{df['FraudResult'].value_counts(normalize=True)}\")\n",
    "    logging.info(f\"First 5 rows:\\n{df.head().to_string()}\")\n",
    "    for col in CATEGORICAL_COLS:\n",
    "        logging.info(f\"{col} unique values: {df[col].nunique()}\")\n",
    "        logging.info(f\"{col} values: {df[col].unique().tolist()}\")\n",
    "    \n",
    "    # Validate required columns\n",
    "    required_cols = ['CustomerId', 'Amount', 'TransactionStartTime', 'FraudResult'] + \\\n",
    "                   ['Value', 'CountryCode', 'PricingStrategy', 'CurrencyCode', \n",
    "                    'ProductCategory', 'ChannelId', 'ProviderId', 'ProductId']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        logging.error(f\"Missing columns: {missing_cols}\")\n",
    "        raise ValueError(f\"Missing columns: {missing_cols}\")\n",
    "    \n",
    "    # Clean ProductId\n",
    "    df['ProductId'] = df['ProductId'].str.strip().str.lower()\n",
    "    logging.info(f\"Cleaned ProductId unique values: {df['ProductId'].nunique()}\")\n",
    "    \n",
    "    # Check for unexpected ProductId values and group them\n",
    "    unexpected = set(df['ProductId'].unique()) - set(EXPECTED_PRODUCT_IDS)\n",
    "    if unexpected:\n",
    "        logging.warning(f\"Found unexpected ProductId values. Grouping into 'other'.\")\n",
    "        df['ProductId'] = df['ProductId'].where(df['ProductId'].isin(EXPECTED_PRODUCT_IDS), 'other')\n",
    "    \n",
    "    # Get ProductId categories for the encoder\n",
    "    product_id_categories = sorted(df['ProductId'].unique().tolist())\n",
    "    logging.info(f\"ProductId categories for OneHotEncoder: {product_id_categories}\")\n",
    "    \n",
    "    # Impute missing values for specified categorical columns\n",
    "    df[IMPUTE_CATEGORICAL_COLS] = df[IMPUTE_CATEGORICAL_COLS].fillna('Unknown')\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Create and fit the feature engineering pipeline\n",
    "    logging.info(\"Fitting and transforming data...\")\n",
    "    pipeline = create_feature_engineering_pipeline(product_id_categories)\n",
    "    try:\n",
    "        # The custom transformers (AggregateFeatures, DatetimeFeatures) are called first\n",
    "        # before the preprocessor, so X is modified in place within the pipeline.\n",
    "        X_with_features = pipeline.named_steps['datetime'].transform(\n",
    "            pipeline.named_steps['aggregate'].transform(X)\n",
    "        )\n",
    "        # Now apply the preprocessor\n",
    "        X_transformed = pipeline.named_steps['preprocessor'].fit_transform(X_with_features)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Pipeline transformation failed: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    # Get feature names directly from the fitted preprocessor\n",
    "    # This is the corrected and robust way to get feature names\n",
    "    feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "    \n",
    "    # Log feature names and shapes for debugging\n",
    "    logging.info(f\"Total feature names count: {len(feature_names)}\")\n",
    "    logging.info(f\"Feature names: {feature_names.tolist()}\")\n",
    "    logging.info(f\"Transformed data shape: {X_transformed.shape}\")\n",
    "    \n",
    "    # Validate shape\n",
    "    if X_transformed.shape[1] != len(feature_names):\n",
    "        logging.error(f\"Shape mismatch detected. Expected {len(feature_names)} columns, got {X_transformed.shape[1]}.\")\n",
    "        raise ValueError(f\"Shape mismatch: Transformed data has {X_transformed.shape[1]} columns, but feature_names has {len(feature_names)}\")\n",
    "    \n",
    "    # Convert transformed data to DataFrame\n",
    "    X_transformed_df = pd.DataFrame(X_transformed, columns=feature_names)\n",
    "    X_transformed_df[target_col] = y.reset_index(drop=True)\n",
    "    \n",
    "    # Save transformed data\n",
    "    logging.info(f\"Saving transformed data to {output_path}...\")\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    X_transformed_df.to_csv(output_path, index=False)\n",
    "    logging.info(f\"Transformed data saved to {output_path}\")\n",
    "    \n",
    "    return X_transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1393245e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 15:24:29,524 - INFO - Checking for input file at ../data/raw/data.csv...\n",
      "2025-07-02 15:24:29,536 - INFO - Loading data from ../data/raw/data.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 15:24:30,162 - INFO - Dataset info:\n",
      "None\n",
      "2025-07-02 15:24:30,170 - INFO - ProviderId unique values: 6\n",
      "2025-07-02 15:24:30,177 - INFO - ProductId unique values: 23\n",
      "2025-07-02 15:24:30,188 - INFO - FraudResult distribution:\n",
      "FraudResult\n",
      "0    0.997982\n",
      "1    0.002018\n",
      "Name: proportion, dtype: float64\n",
      "2025-07-02 15:24:30,200 - INFO - First 5 rows:\n",
      "         TransactionId         BatchId       AccountId       SubscriptionId       CustomerId CurrencyCode  CountryCode    ProviderId     ProductId     ProductCategory    ChannelId   Amount  Value  TransactionStartTime  PricingStrategy  FraudResult\n",
      "0  TransactionId_76871   BatchId_36123  AccountId_3957   SubscriptionId_887  CustomerId_4406          UGX          256  ProviderId_6  ProductId_10             airtime  ChannelId_3   1000.0   1000  2018-11-15T02:18:49Z                2            0\n",
      "1  TransactionId_73770   BatchId_15642  AccountId_4841  SubscriptionId_3829  CustomerId_4406          UGX          256  ProviderId_4   ProductId_6  financial_services  ChannelId_2    -20.0     20  2018-11-15T02:19:08Z                2            0\n",
      "2  TransactionId_26203   BatchId_53941  AccountId_4229   SubscriptionId_222  CustomerId_4683          UGX          256  ProviderId_6   ProductId_1             airtime  ChannelId_3    500.0    500  2018-11-15T02:44:21Z                2            0\n",
      "3    TransactionId_380  BatchId_102363   AccountId_648  SubscriptionId_2185   CustomerId_988          UGX          256  ProviderId_1  ProductId_21        utility_bill  ChannelId_3  20000.0  21800  2018-11-15T03:32:55Z                2            0\n",
      "4  TransactionId_28195   BatchId_38780  AccountId_4841  SubscriptionId_3829   CustomerId_988          UGX          256  ProviderId_4   ProductId_6  financial_services  ChannelId_2   -644.0    644  2018-11-15T03:34:21Z                2            0\n",
      "2025-07-02 15:24:30,256 - INFO - CurrencyCode unique values: 1\n",
      "2025-07-02 15:24:30,263 - INFO - CurrencyCode values: ['UGX']\n",
      "2025-07-02 15:24:30,272 - INFO - ProductCategory unique values: 9\n",
      "2025-07-02 15:24:30,282 - INFO - ProductCategory values: ['airtime', 'financial_services', 'utility_bill', 'data_bundles', 'tv', 'transport', 'ticket', 'movies', 'other']\n",
      "2025-07-02 15:24:30,295 - INFO - ChannelId unique values: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 95662 entries, 0 to 95661\n",
      "Data columns (total 16 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   TransactionId         95662 non-null  object \n",
      " 1   BatchId               95662 non-null  object \n",
      " 2   AccountId             95662 non-null  object \n",
      " 3   SubscriptionId        95662 non-null  object \n",
      " 4   CustomerId            95662 non-null  object \n",
      " 5   CurrencyCode          95662 non-null  object \n",
      " 6   CountryCode           95662 non-null  int64  \n",
      " 7   ProviderId            95662 non-null  object \n",
      " 8   ProductId             95662 non-null  object \n",
      " 9   ProductCategory       95662 non-null  object \n",
      " 10  ChannelId             95662 non-null  object \n",
      " 11  Amount                95662 non-null  float64\n",
      " 12  Value                 95662 non-null  int64  \n",
      " 13  TransactionStartTime  95662 non-null  object \n",
      " 14  PricingStrategy       95662 non-null  int64  \n",
      " 15  FraudResult           95662 non-null  int64  \n",
      "dtypes: float64(1), int64(4), object(11)\n",
      "memory usage: 11.7+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 15:24:30,348 - INFO - ChannelId values: ['ChannelId_3', 'ChannelId_2', 'ChannelId_1', 'ChannelId_5']\n",
      "2025-07-02 15:24:30,359 - INFO - ProviderId unique values: 6\n",
      "2025-07-02 15:24:30,369 - INFO - ProviderId values: ['ProviderId_6', 'ProviderId_4', 'ProviderId_1', 'ProviderId_5', 'ProviderId_3', 'ProviderId_2']\n",
      "2025-07-02 15:24:30,493 - INFO - Cleaned ProductId unique values: 23\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'EXPECTED_PRODUCT_IDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m default_output_path = \u001b[33m'\u001b[39m\u001b[33m../data/processed/transformed_transactions.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     transformed_df = \u001b[43mprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefault_input_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_output_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m      7\u001b[39m     logging.info(\u001b[33m\"\u001b[39m\u001b[33mPlease ensure the data.csv file is in the correct directory or provide the correct path.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mprocess_data\u001b[39m\u001b[34m(input_path, output_path, target_col)\u001b[39m\n\u001b[32m     41\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCleaned ProductId unique values: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[33m'\u001b[39m\u001b[33mProductId\u001b[39m\u001b[33m'\u001b[39m].nunique()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Check for unexpected ProductId values and group them\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m unexpected = \u001b[38;5;28mset\u001b[39m(df[\u001b[33m'\u001b[39m\u001b[33mProductId\u001b[39m\u001b[33m'\u001b[39m].unique()) - \u001b[38;5;28mset\u001b[39m(\u001b[43mEXPECTED_PRODUCT_IDS\u001b[49m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m unexpected:\n\u001b[32m     46\u001b[39m     logging.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound unexpected ProductId values. Grouping into \u001b[39m\u001b[33m'\u001b[39m\u001b[33mother\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'EXPECTED_PRODUCT_IDS' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    default_input_path = '../data/raw/data.csv'\n",
    "    default_output_path = '../data/processed/transformed_transactions.csv'\n",
    "    try:\n",
    "        transformed_df = process_data(default_input_path, default_output_path)\n",
    "    except FileNotFoundError:\n",
    "        logging.info(\"Please ensure the data.csv file is in the correct directory or provide the correct path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b047d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
